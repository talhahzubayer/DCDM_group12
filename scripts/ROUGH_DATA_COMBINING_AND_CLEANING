 ```{r}
# Merge all tables into one big table

```{r}
# Set your folder path for transposed files
transposed_folder <- "/Users/sabrinasaidoune/Desktop/Group12/data/phenotype_analysis_results_transposed"

# List all transposed CSV files
file_list <- list.files(path = transposed_folder, pattern = "*.csv", full.names = TRUE)

# Read all files into a list
df_list <- lapply(file_list, function(file) {
  read.csv(file, stringsAsFactors = FALSE)
})

# Define the correct 8 columns
expected_cols <- c("gene_accession_id", "gene_symbol", "mouse_strain", 
                   "mouse_life_stage", "parameter_id", "pvalue", 
                   "parameter_name", "analysis_id")

# Align all data frames to have these exact columns
df_list_aligned <- lapply(df_list, function(df) {
  # Clean up column name spaces
  names(df) <- trimws(names(df))
  
  # Add missing columns as NA
  missing_cols <- setdiff(expected_cols, names(df))
  for (col in missing_cols) {
    df[[col]] <- NA
  }
  
  # Keep only the expected columns (ignore extras)
  df <- df[expected_cols]
  
  return(df)
})

# Combine all aligned data frames
combined_data <- do.call(rbind, df_list_aligned)

# Print summary info
cat("Combined table created with", nrow(combined_data), "rows and", ncol(combined_data), "columns.\n")

# Save to a new CSV file
write.csv(combined_data, 
          file = file.path(transposed_folder, "data_table_combined.csv"), 
          row.names = FALSE)

```


# Set folder path
group_folder <- "/Users/sabrinasaidoune/Desktop/DCDM_CW1_GROUP12/Group12/"

# Load the original combined table
combined_file <- file.path(group_folder, "data_table_combined.csv")
combined_data <- read.csv(combined_file, stringsAsFactors = FALSE)

# Create a copy for cleaning
cleaned_data <- combined_data

# Initialize a logical vector to track valid rows
valid_rows <- rep(TRUE, nrow(cleaned_data))

# Helper function to update valid_rows
check_length <- function(column, min_len, max_len) {
  nchar_val <- nchar(as.character(column))
  return(nchar_val >= min_len & nchar_val <= max_len)
}

# Apply SOP rules
valid_rows <- valid_rows & (nchar(as.character(cleaned_data$analysis_id)) == 15)
valid_rows <- valid_rows & check_length(cleaned_data$gene_accession_id, 9, 11)
valid_rows <- valid_rows & check_length(cleaned_data$gene_symbol, 1, 13)
valid_rows <- valid_rows & check_length(cleaned_data$mouse_strain, 3, 6)
valid_rows <- valid_rows & check_length(cleaned_data$mouse_life_stage, 4, 17)
valid_rows <- valid_rows & check_length(cleaned_data$parameter_id, 15, 20)
valid_rows <- valid_rows & check_length(cleaned_data$parameter_name, 2, 74)
cleaned_data$pvalue <- as.numeric(cleaned_data$pvalue)
valid_rows <- valid_rows & (cleaned_data$pvalue >= 0 & cleaned_data$pvalue <= 1)

# Split the data into cleaned and removed tables
final_cleaned <- cleaned_data[valid_rows, ]
removed_data <- cleaned_data[!valid_rows, ]

# Save the cleaned table
write.csv(final_cleaned, file.path(group_folder, "data_table_cleaned.csv"), row.names = FALSE)

# Save the removed/erased rows
write.csv(removed_data, file.path(group_folder, "data_table_removed.csv"), row.names = FALSE)

cat("cleaning complete.\n")
cat("Remaining rows (cleaned):", nrow(final_cleaned), "\n")
cat("Removed rows (failed SOP):", nrow(removed_data), "\n")

```

```{r}
# Set folder path
group_folder <- "/Users/sabrinasaidoune/Desktop/DCDM_CW1_GROUP12/Group12/"

# Load the cleaned data
cleaned_file <- file.path(group_folder, "data_table_cleaned.csv")
cleaned_data <- read.csv(cleaned_file, stringsAsFactors = FALSE)

# Remove any rows containing NA (creates a new dataset)
noNA_data <- na.omit(cleaned_data)

# Save the new table as a separate file
write.csv(noNA_data, file.path(group_folder, "data_table_noNA.csv"), row.names = FALSE)

# Print summary
cat(" New table created without NAs. Original table remains untouched.\n")
cat("Rows remaining:", nrow(noNA_data), "\n")

```

# further cleaning to remove duplicates and outliers

```{r}
# Set folder path
group_folder <- "/Users/sabrinasaidoune/Desktop/DCDM_CW1_GROUP12/Group12/"

# Load the no-NA dataset
noNA_file <- file.path(group_folder, "data_table_noNA.csv")
noNA_data <- read.csv(noNA_file, stringsAsFactors = FALSE)

# ---- Step 1: Remove duplicates ----
# Remove duplicate rows based on all columns
dedup_data <- noNA_data[!duplicated(noNA_data), ]

cat("Duplicates removed:", nrow(noNA_data) - nrow(dedup_data), "\n")

# ---- Step 2: Detect and remove outliers ----
# We'll remove outliers in numeric columns only
numeric_cols <- sapply(dedup_data, is.numeric)

# Function to flag outliers using IQR method
remove_outliers <- function(x) {
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR_val <- Q3 - Q1
  # Values within 1.5 * IQR from Q1 and Q3 are kept
  x >= (Q1 - 1.5 * IQR_val) & x <= (Q3 + 1.5 * IQR_val)
}

# Initialize a logical vector of TRUEs
keep_rows <- rep(TRUE, nrow(dedup_data))

# Apply outlier filter for all numeric columns
for (col in names(dedup_data)[numeric_cols]) {
  valid <- remove_outliers(dedup_data[[col]])
  keep_rows <- keep_rows & valid
}

# Filter data
cleaned_final <- dedup_data[keep_rows, ]

cat("Outliers removed:", nrow(dedup_data) - nrow(cleaned_final), "\n")

# ---- Step 3: Save the final cleaned table ----
output_file <- file.path(group_folder, "data_table_noNA_noDup_noOutliers.csv")
write.csv(cleaned_final, output_file, row.names = FALSE)

# ---- Step 4: Summary ----
cat("Further cleaning complete.\n")
cat("Rows remaining after removing duplicates and outliers:", nrow(cleaned_final), "\n")
cat("Final cleaned file saved as:", output_file, "\n")

### 1. Load the original CSV file
df <- read.csv(
  "clean_table_final.csv",
  header = TRUE,
  stringsAsFactors = FALSE
)

### 2. Make a copy so the original stays unchanged
new_df <- df

### 3. Convert the gene_accession_id column to ALL CAPS
new_df$gene_accession_id <- toupper(new_df$gene_accession_id)

### 4. Save the modified data as a new CSV file
write.csv(
  new_df,
  "clean_table_final_UPPERCASE.csv",
  row.names = FALSE
)
```
